{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BitViT - Example Fine-tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training script\n",
    "%run -i train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "config = {\n",
    "    \"project\": \"distill-bitvit-finetune\",\n",
    "    \"experiment\": \"bit-vit\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"accum_steps\": 1, # gradient accumulation steps\n",
    "    \"warmup\": 5, # in epochs\n",
    "    \"finetune\": {\n",
    "        \"enabled\": True,\n",
    "        \"project\": \"distill-bitvit-pretrain\",\n",
    "        \"experiment\": \"bit-vit\",\n",
    "        \"args\": {\n",
    "            \"unfreeze_blocks\": 6,\n",
    "            \"hidden_sizes\": [192, 96, 48, 24],\n",
    "            \"dropout\": 0.2,\n",
    "        },\n",
    "    },\n",
    "    \"distill\": {\n",
    "        \"enabled\": True,\n",
    "        \"args\": {\n",
    "            \"teacher\": efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1),\n",
    "            \"hard\": True,  # use hard or soft labels\n",
    "            \"alpha\": 0.5,  # trade-off between main loss and distillation loss\n",
    "            \"temperature\": 1.0,  # only used for soft distillation\n",
    "        },\n",
    "    }, \n",
    "    \"architecture\": {\n",
    "        \"model\": DistillableBitViT,\n",
    "        \"args\": { # based on DeiT-Small\n",
    "            \"image_size\": 224,\n",
    "            \"patch_size\": 16,\n",
    "            \"num_classes\": 1000,\n",
    "            \"dim\": 384,\n",
    "            \"depth\": 12,\n",
    "            \"heads\": 6,\n",
    "            \"mlp_dim\": 1536,\n",
    "            \"spt\": False, # from vits for small datasets\n",
    "            \"sincos2d\": True, # from SimpleViT\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": torch.optim.AdamW,\n",
    "        \"args\": {  # based on SimpleViT\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        },\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "        \"args\": {\n",
    "            \"T_max\": 10,\n",
    "        },\n",
    "    },\n",
    "    \"criterion\": { # disabled for distillation\n",
    "        \"type\": nn.CrossEntropyLoss,\n",
    "        \"args\": {},\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"path\": f\"{os.environ['TMPDIR']}/datasets/celeb-df-v2\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"preprocess\": {\n",
    "            \"train\": transforms.Compose(\n",
    "                [\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(), # u can add more augmentations here\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            \"validation\": transforms.Compose(\n",
    "                [\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        },\n",
    "        \"cutmix_or_mixup\": {\n",
    "            \"enabled\": False,\n",
    "        },\n",
    "        \"loader\": {\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": 8,\n",
    "            \"prefetch_factor\": 4,\n",
    "            \"pin_memory\": True,\n",
    "            \"drop_last\": True,\n",
    "            \"persistent_workers\": True,\n",
    "        }\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"enabled\": True,\n",
    "        \"log_interval\": 10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fine-tuning or resume from last checkpoint if available\n",
    "finetune_or_resume(config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
